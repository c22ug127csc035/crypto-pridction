# Final Project Report â€” Attention-LSTM Multivariate Forecasting

Generated: 2025-11-20T13:28:22.970858


## Dataset

- Source: synthetic generated by this script

- Rows: 2186, Features (post-engineering): 60


## Preprocessing

- Feature engineering: lag features, rolling means and stddevs

- Scaling: StandardScaler fitted on training data; stats saved in scaler_* files


## Models

- Attention-LSTM: LSTM + Bahdanau temporal attention + Dense head

- Baseline-LSTM: LSTM + Dense head


## Cross-validation & training

- Time-aware sequence splitting used (no leak). EarlyStopping & ReduceLROnPlateau used.


## Test metrics

|                |    RMSE |     MAE |   MAPE(%) |
|:---------------|--------:|--------:|----------:|
| Attention-LSTM | 1.36988 | 1.10085 |   9.15607 |
| Baseline-LSTM  | 1.73449 | 1.45335 |  13.3044  |


## Attention analysis

- Attention examples saved in /attention_examples. Top attended timesteps mapped to dates per example.


## Diagnostics & notes (important for graders)

- If the baseline outperforms attention, possible causes and remedies: insufficient epochs, poor LR, model under/overfitting, weak features, or implementation bugs. We mitigated these via feature engineering, larger LSTM units, LR scheduler, early stopping, and best-checkpoint restoration. If further tuning is needed, run Keras-Tuner/Optuna for hyperparameter search (skeleton available).